# -*- coding: utf-8 -*-
"""01_pytorch_workflow_video_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_KsJ7H9ylv3QkUe6LQsWADGiVf0KXQS8
"""

what_were_covering = {1: "data (prepare and load)",
                      2: "build model",
                      3: "fitting the model to data (training) ",
                      4: "making predictions & evaluating a model (inference)",
                      5: "saving & loading a model",
                      6: "putting it all together" }

what_were_covering

import torch
from torch import nn ## nn contains all of PyTorch's building blocks for neural networks.
import matplotlib.pyplot as plt

# Check PyTorch version
torch.__version__

"""## 1. Data (Preparing & Loading)

In ML, data can be almost anything eg:

* Excel spreadsheet
* Images of any kind
* Videos
* Audio

ML:

1. Get data into numerical representation
2. Build models to learn patterns in that numerical representation

Linear Regression Formula:

Making a straight line with known **parameters**
"""

#Creating known parameters

weight = 0.7
bias = 0.3

start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim = 1)
y = weight * X + bias

X[:10], y[:10]

len(X), len(y)

"""### Splitting data into training & test sets (VERY IMPORTANT)"""

#Create train/test split

train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)

def plot_predictions(train_data = X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_labels = y_test,
                     predictions = None):
      """
      Plots training data, test data & compares predictions.
      """
      plt.figure(figsize = (10, 7))

      # Plot training in blue
      plt.scatter(train_data, train_labels, c = "b", s = 4, label = "Training data")

      # Plot test data in green

      plt.scatter(test_data, test_labels, c = "g", s = 4, label = "Testing data")

      # Are there predictions?
      if predictions is not None:
        plt.scatter(test_data, predictions, c = "r", s = 4, label = "Predictions")

      plt.legend(prop = {"size" : 14});

plot_predictions();

"""## Building a PyTorch model


"""

# Create linear regression model class

from torch import nn

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.rand(1,
                                           requires_grad = True,
                                           dtype = torch.float))

    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad = True,
                                         dtype = torch.float))

  # Forward method to show computation in model.

  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias

"""### PyTorch model building essentials

* torch.nn - contains all building blocks for computational graphs (eg: neural networks )
* torch.nn.Parameter
* torch.nn.Module - Base class for all neural networks
* torch.optim - Location for PyTorch Live
* def forward() - All nn.module subclasses require you to overwrite forward().




"""

# Random seed:
import torch
from torch import nn

torch.manual_seed(42)

model_0 = LinearRegressionModel()

list(model_0.parameters())

torch.manual_seed(42)
torch.randn(1)

# List named parameters

model_0.state_dict()

weight, bias

X_test, y_test

# Checking our model's predictive power:

with torch.inference_mode():
  y_preds = model_0(X_test)




y_preds

y_preds = model_0(X_test)
y_preds

plot_predictions(predictions = y_preds.detach().cpu().numpy())

"""### Training model

Training model - allowing model to move from an unknown parameters (random) to known parameters.

"""

list(model_0.parameters())

# Check model's parameters (a value that the model sets itself)

model_0.state_dict()

# Setup loss function

loss_fn = nn.L1Loss()

optimizer = torch.optim.SGD(params = model_0.parameters(),
                            lr = 0.1) # lr = learning rate

with torch.no_grad():
  list(model_0.parameters())

list(model_0.parameters())

torch.manual_seed(42)

epochs = 200

# Track different values
epoch_count = []
loss_values = []
test_loss_values = []

for epoch in range(epochs):

  model_0.train()

  #1) Forward pass
  y_pred =  model_0(X_train)

  loss = loss_fn(y_pred, y_train)


  optimizer.zero_grad()

  loss.backward()

  optimizer.step() # How the optimizer changes will accumate through the loop.


  # Testing
  model_0.eval() # turns off different settings in the model not needed for evaluation/testing (droupout/batch norm layers)
  with torch.inference_mode(): # turns off gradient tracking.
  # with torch.no_grad(): Slower way of torch.inference_mode()

    test_pred = model_0(X_test)

    test_loss = loss_fn(test_pred, y_test)

  if epoch % 10 == 0:

    epoch_count.append(epoch)
    loss_values.append(loss)
    test_loss_values.append(test_loss)

    print(f"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}")

    print(f"Epoch {epoch}: Train loss = {loss:.4f}, Test loss = {test_loss:.4f}")







    print(model_0.state_dict())

import numpy as np
np.array(torch.tensor(loss_values).numpy()), test_loss_values

# Plot loss curves

plt.plot(epoch_count, np.array(torch.tensor(loss_values).cpu().numpy()), label = "Train loss")
plt.plot(epoch_count, test_loss_values, label = "Test loss")
plt.title("Training & test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();

with torch.inference_mode():
  y_preds_new = model_0(X_test)

model_0.state_dict()

weight, bias

plot_predictions(predictions = y_preds.detach().cpu().numpy())

plot_predictions(predictions = y_preds_new)

"""## Saving a model

3 main methods:

torch.save() - Save PyTorch objects in pickle format.

torch.load() - Load a saved PyTorch object

torch.nn.Module.load_state_dict() - Load model's saved state dictionary

"""

# Save PyTorch model

from pathlib import Path

MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents = True, exist_ok = True)

MODEL_NAME = "01_pytorch_workflow_model_0.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

MODEL_SAVE_PATH

print(f"Saving model to : {MODEL_SAVE_PATH}")
torch.save(obj = model_0.state_dict(),
           f = MODEL_SAVE_PATH)

!ls -l models

## Loading PyTorch

loaded_model_0 = LinearRegressionModel()

loaded_model_0.load_state_dict(torch.load(f = MODEL_SAVE_PATH))

loaded_model_0.state_dict()

loaded_model_0.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_0(X_test)

loaded_model_preds

y_preds == loaded_model_preds

y_preds

model_0.eval()
with torch.inference_mode():
  y_preds = model_0(X_test)

y_preds

"""Putting it all together:"""

import torch
from torch import nn
import matplotlib.pyplot as plt

# Check PyTorch version:

torch.__version__

"""Create Device-Agnostic Code

If there is access to a GPU, our code will use it for potentially faster computing.

If there isn't a GPU available, code will default to using CPU.

"""

# Setup device agnostic code

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

!nvidia-smi

"""# 6.1 Data"""

# Create data using linear regression formula y = weight * X + bias (y = mx + c)

weight = 0.7
bias = 0.3

# Create range values

start = 0
end = 1
step = 0.02

# Create X & y (features & labels)

X = torch.arange(start, end, step).unsqueeze(dim = 1) # without unsqueeze, errors pop up
y = weight * X + bias
X[:10], y[:10]

# Split data

train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
len(X_train), len(y_train), len(X_test), len(y_test)

# Plot the data
# Note: If plot_predictions() function not loaded, error will occur.

plot_predictions(X_train, y_train, X_test, y_test)

"""## Building a PyTorch Linear Model"""

# Create linear model by subclassing nn.Module

class LinearRegressionModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    # Use nn.Linear() for creating the model parameters / linear transform, probing layer, fully connected layer, dense layer
    self.linear_layer = nn.Linear(in_features = 1,
                                  out_features = 1)

  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)

  # Set manual seed

torch.manual_seed(42)
model_1 = LinearRegressionModelV2()
model_1, model_1.state_dict()

X_train[:5], y_train[:5]

# Check model current device
next(model_1.parameters()).device

# Set model to use the target device

model_1.to(device)
next(model_1.parameters()).device

"""###6.3 Training

For training we need:
* Loss function
* Optimizer
* Training loop
* Test loop
"""

# Setup loss function
loss_fn = nn.L1Loss() # same as MAE

# Setup optimizer
optimizer = torch.optim.SGD(params = model_1.parameters(),
                            lr = 0.01, )

# Training loop:

torch.manual_seed(42)

epochs = 200

# Put data on the target device (device agnostic code for data)
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

for epoch in range (epochs):
  model_1.train()

  # 1. Forward pass
  y_pred = model_1(X_train)

  # 2. Calculate loss
  loss = loss_fn(y_pred, y_train)

  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Perform backpropagation
  loss.backward()

  # 5. Optimizer step
  optimizer.step()

  ### Testing
  model_1.eval()
  with torch.inference_mode():
    test_pred = model_1(X_test)

    test_loss = loss_fn(test_pred, y_test)

  # Print out what's happening

  if epoch % 10 == 0:
    print(f"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}")

model_1.state_dict()

weight, bias

"""### 6.4 Making & Evaluating Predictions"""

# Turn model into evaluation mode
model_1.eval()

# Make predictions on the test data
with torch.inference_mode():
  y_preds = model_1(X_test)
y_preds

# Check out our model predictions visually
plot_predictions(predictions = y_preds.cpu())

"""### 6.5 Saving & loading a trained model"""

from pathlib import Path

# 1. Create models directory

MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents = True, exist_ok = True)

# 2. Create model save path

MODEL_NAME = "01_pytorch_workflow_model_1.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Save model state dict

print(f"Saving model to : {MODEL_SAVE_PATH}")
torch.save(obj = model_1.state_dict(),
           f = MODEL_SAVE_PATH)

model_1.state_dict()

# Load a PyTorch Model

# Create a new instance of Linear Regression Model V2
loaded_model_1 = LinearRegressionModelV2()

# Load saved model_1 state_dict
loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))

# Put the loaded model to device
loaded_model_1.to(device)

next(loaded_model_1.parameters()).device

loaded_model_1.state_dict()

# Evaluate loaded model
loaded_model_1.eval()
with torch.inference_mode():
  loaded_model_1_preds = loaded_model_1(X_test)
y_preds == loaded_model_1_preds

"""## Exercises & Extra Curriculum

Pytorch Deep Learning repo https://www.learnpytorch.io/01_pytorch_workflow/
"""

